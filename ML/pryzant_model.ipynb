{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pryzant_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCWuDTV8lOYg"
      },
      "source": [
        "# Imports and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OQGJL7mGeth",
        "outputId": "4aa37a1f-302d-4622-dfde-11057420e3e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install pytorch-pretrained-bert\n",
        "!pip3 install simplediff\n",
        "!pip3 install tensorboardX"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/54/099a2ea5d4b2d5931a26f280a7585f613b1fafaac9189e489a9e25004a01/boto3-1.16.13-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Collecting botocore<1.20.0,>=1.19.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/40/b5e681d80dc46bafd0dc2e55266190cc432dfd5b72b9e7e1c5743aa6c362/botocore-1.19.13-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 8.6MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.13->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.13->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.13 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.13 botocore-1.19.13 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n",
            "Collecting simplediff\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/8a/935c48a9364d0d6773ec90a1c042a151ff2f08b99ae1c79b0e08ab01d4a2/simplediff-1.0.tar.gz\n",
            "Building wheels for collected packages: simplediff\n",
            "  Building wheel for simplediff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplediff: filename=simplediff-1.0-cp36-none-any.whl size=4438 sha256=3920c8e3b505043384346dd87b650a282282959e468d933144214799076d6770\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/74/06/04375013eda8bc3573dfa4fc9fe0cf4ae7c6dbb8812b26f186\n",
            "Successfully built simplediff\n",
            "Installing collected packages: simplediff\n",
            "Successfully installed simplediff-1.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScTU41vvPPO3"
      },
      "source": [
        "#from pytorch_pretrained_bert.modeling import PreTrainedBertModel, BertModel, BertSelfAttention\n",
        "import pytorch_pretrained_bert.modeling as modeling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertForTokenClassification\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tensorboardX import SummaryWriter\n",
        "import argparse\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AP37WF_N2A8"
      },
      "source": [
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertSelfAttention\n",
        "from pytorch_pretrained_bert.modeling import BertPreTrainedModel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vKmETI1i79"
      },
      "source": [
        "## Data Study\n",
        "Raw data files contain the following: \n",
        "\n",
        "\n",
        "318427508\t\n",
        "\n",
        "in april 2009 a brazilian human rights group , torture never again , awarded the five its chico mendes medal , because their rights had been violated .\n",
        "\n",
        "in april 2009 a brazilian human rights group , torture never again , awarded the five its chico mendes medal , under the pre ##text that their rights had been violated .\n",
        "\n",
        "in april 2009 a brazilian human rights group, torture never again, awarded the five its chico mendes medal, because their rights had been violated.\n",
        "\n",
        "in april 2009 a brazilian human rights group, torture never again, awarded the five its chico mendes medal, under the pretext that their rights had been violated.\n",
        "\n",
        "ADP NOUN NUM DET ADJ ADJ NOUN NOUN PUNCT NOUN ADV ADV PUNCT VERB DET NUM ADJ NOUN NOUN NOUN PUNCT ADP ADJ NOUN VERB VERB VERB PUNCT\t\n",
        "\n",
        "prep pobj nummod det amod amod compound nsubj punct appos neg advmod punct ROOT det nummod poss compound compound dobj punct mark poss nsubjpass aux auxpass advcl punct\n",
        "\n",
        "--------------------\n",
        "\n",
        "235640083\t\n",
        "\n",
        "the 51 day stand ##off and ensuing murder of 76 men , women , and children - - the branch david ##ians - - in wa ##co , texas .\t\n",
        "\n",
        "the 51 day stand ##off and ensuing deaths of 76 men , women , and children - - the branch david ##ians - - in wa ##co , texas .\n",
        "\n",
        "the 51 day standoff and ensuing murder of 76 men, women, and children--the branch davidians--in waco, texas.\t\n",
        "\n",
        "the 51 day standoff and ensuing deaths of 76 men, women, and children--the branch davidians--in waco, texas.\t\n",
        "\n",
        "DET NUM NOUN NOUN NOUN CCONJ VERB NOUN ADP NUM NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT PUNCT DET NOUN NOUN NOUN PUNCT PUNCT PART NOUN NOUN PUNCT NOUN PUNCT\t\n",
        "\n",
        "det nummod compound nsubj nsubj cc amod conj prep nummod pobj punct conj punct cc conj punct punct det nsubj conj conj punct punct prep pobj pobj punct ROOT punct\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnFbXR3aVbUT",
        "outputId": "d88baab2-68ad-4a3b-e17a-17c6e4da922e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Universal Dependencies Scheme used in all languages trained on Universal Dependency Corpora\n",
        "# some english dependency labels use CLEAR style by ClearNLP\n",
        "# SYNTACTIC DEPENDENCIES\n",
        "RELATIONS = [\n",
        "  'det', # determiner (the, a)\n",
        "  'amod', # adjectival modifier\n",
        "  'nsubj', # nominal subject\n",
        "  'prep', # prepositional modifier\n",
        "  'pobj', # object of preposition\n",
        "  'ROOT', # root\n",
        "  'attr', # attribute\n",
        "  'punct', # punctuation\n",
        "  'advmod', # adverbial modifier\n",
        "  'compound', # compound\n",
        "  'acl', # clausal modifier of noun (adjectivial clause)\n",
        "  'agent', # agent\n",
        "  'aux', # auxiliary\n",
        "  'ccomp', # clausal complement\n",
        "  'dobj', # direct object\n",
        "  'cc', # coordinating conjunction \n",
        "  'conj', # conjunct\n",
        "  'appos', # appositional \n",
        "  'nsubjpass', # nsubjpass\n",
        "  'auxpass', # auxiliary (passive)\n",
        "  'poss', # poss\n",
        "  'nummod', # numeric modifier\n",
        "  'nmod', # nominal modifier\n",
        "  'relcl', # relative clause modifier\n",
        "  'mark', # marker\n",
        "  'advcl', # adverbial clause modifier\n",
        "  'pcomp', # complement of preposition\n",
        "  'npadvmod', # noun phrase as adverbial modifier\n",
        "  'preconj', # pre-correlative conjunction\n",
        "  'neg', # negation modifier\n",
        "  'xcomp', # open clausal complement\n",
        "  'csubj', # clausal subject\n",
        "  'prt', # particle\n",
        "  'parataxis', # parataxis\n",
        "  'expl', # expletive\n",
        "  'case', # case marking\n",
        "  'acomp', # adjectival complement\n",
        "  'predet', # ??? \n",
        "  'quantmod', # modifier of quantifier\n",
        "  'dep', # unspecified dependency\n",
        "  'oprd', # object predicate\n",
        "  'intj', # interjection\n",
        "  'dative', # dative\n",
        "  'meta', # meta modifier\n",
        "  'csubjpass', # clausal subject (passive)\n",
        "  '<UNK>' # unknown\n",
        "]\n",
        "\n",
        "REL2ID = {x: i for i,x in enumerate(RELATIONS)}\n",
        "\n",
        "# PARTS OF SPEECH\n",
        "POS_TAGS = [\n",
        "  'DET', # determiner (a, an, the)\n",
        "  'ADJ', # adjective (big, old, green, first)\n",
        "  'NOUN', # noun (girl, cat, tree)\n",
        "  'ADP', # adposition (in, to, during)\n",
        "  'NUM', # numeral (1, 2017, one, IV)\n",
        "  'VERB', # verb (runs, running, eat, ate)\n",
        "  'PUNCT', # punctuation (., (, ), ?)\n",
        "  'ADV', # adverb (very, tomorrow, down)\n",
        "  'PART', # particle ('s, not)\n",
        "  'CCONJ', # coordinating conjunction (and, or, but)\n",
        "  'PRON', # pronoun(I, you, he, she)\n",
        "  'X', # other (fhefkoskjsdods)\n",
        "  'INTJ', # interjection (hello, psst, ouch, bravo)\n",
        "  'PROPN', # proper noun (Mary, John, London, HBO) \n",
        "  'SYM', # symbol ($, %, +, -, =)\n",
        "  '<UNK>' # unknown\n",
        "]\n",
        "\n",
        "POS2ID = {x: i for i, x in enumerate(POS_TAGS)}\n",
        "POS2ID"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 15,\n",
              " 'ADJ': 1,\n",
              " 'ADP': 3,\n",
              " 'ADV': 7,\n",
              " 'CCONJ': 9,\n",
              " 'DET': 0,\n",
              " 'INTJ': 12,\n",
              " 'NOUN': 2,\n",
              " 'NUM': 4,\n",
              " 'PART': 8,\n",
              " 'PRON': 10,\n",
              " 'PROPN': 13,\n",
              " 'PUNCT': 6,\n",
              " 'SYM': 14,\n",
              " 'VERB': 5,\n",
              " 'X': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A08vF7sOv7FU"
      },
      "source": [
        "EDIT_TYPE2ID = {'0':0, '1':1, 'mask':2}\n",
        "\n",
        "# they will add up to 1\n",
        "def softmax(x, axis=None):\n",
        "  x=x-x.max(axis=axis, keepdims=True)\n",
        "  y= np.exp(x)\n",
        "  return y/y.sum(axis=axis, keepdims=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGYyrmKOw66k"
      },
      "source": [
        "# not sure what this function does\n",
        "def get_tok_labels(s_diff):\n",
        "  pre_tok_labels = []\n",
        "  post_tok_labels = []\n",
        "  for tag, chunk in s_diff:\n",
        "    if tag == '=':\n",
        "      pre_tok_labels += [0] * len(chunk)\n",
        "      post_tok_labels += [0] * len(chunk)\n",
        "    elif tag == '-':\n",
        "      pre_tok_labels += [1] * len(chunk) # 1 in pre if word deleted in post\n",
        "    elif tag == '+':\n",
        "      post_tok_labels += [1] * len(chunk) # 1 in post if word added in post\n",
        "    else: \n",
        "      pass\n",
        "  return pre_tok_labels, post_tok_labels \n",
        "  # returns returns list of 0s, list of 1s for both pre and post edit sentences\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy07bm8Oy_u5"
      },
      "source": [
        "# create a randomly sampled noisy version of a sequence\n",
        "# drops out every word in the sentence with a probability p_wd (drop_prob)\n",
        "# slightly shuffle the input sentence \n",
        "def noise_seq(seq, drop_prob=0.25, shuf_dist=3, drop_set=None, keep_bigrams=False):\n",
        "    # from https://arxiv.org/pdf/1711.00043.pdf\n",
        "    def perm(i):\n",
        "        return i[0] + (shuf_dist + 1) * np.random.random()\n",
        "    \n",
        "    if drop_set == None:\n",
        "        dropped_seq = [x for x in seq if np.random.random() > drop_prob]\n",
        "    else:\n",
        "        dropped_seq = [x for x in seq if not (x in drop_set and np.random.random() < drop_prob)]\n",
        "\n",
        "    if keep_bigrams:\n",
        "        i = 0\n",
        "        original = ' '.join(seq)\n",
        "        tmp = []\n",
        "        while i < len(dropped_seq)-1:\n",
        "            if ' '.join(dropped_seq[i : i+2]) in original:\n",
        "                tmp.append(dropped_seq[i : i+2])\n",
        "                i += 2\n",
        "            else:\n",
        "                tmp.append([dropped_seq[i]])\n",
        "                i += 1\n",
        "\n",
        "        dropped_seq = tmp\n",
        "\n",
        "    # global shuffle\n",
        "    if shuf_dist == -1:\n",
        "        shuffle(dropped_seq)\n",
        "    # local shuffle\n",
        "    elif shuf_dist > 0:\n",
        "        dropped_seq = [x for _, x in sorted(enumerate(dropped_seq), key=perm)]\n",
        "    # shuf_dist of 0 = no shuffle\n",
        "\n",
        "    if keep_bigrams:\n",
        "        dropped_seq = [z for y in dropped_seq for z in y]\n",
        "    \n",
        "    return dropped_seq"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ak4eyH30Gpl"
      },
      "source": [
        "# pad end of a sequence so it is the max sequence length\n",
        "def pad(id_arr, pad_idx):\n",
        "  max_seq_len = 80\n",
        "  return id_arr + ([pad_idx] * (max_seq_len - len(id_arr)))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3PbWZrM0Qi3"
      },
      "source": [
        "# get examples of data!\n",
        "def get_examples(data_path, tok2id, max_seq_len, \n",
        "                 noise=False, add_del_tok=False,\n",
        "                 categories_path=None):\n",
        "    #global REL2ID\n",
        "    #global POS2ID\n",
        "    #global EDIT_TYPE2ID\n",
        "\n",
        "    # ARGS.drop_words is not None:\n",
        "    #    drop_set = set([l.strip() for l in open(ARGS.drop_words)])\n",
        "    #else:\n",
        "    #    drop_set = None\n",
        "\n",
        "    skipped = 0 \n",
        "    out = defaultdict(list)\n",
        "    #print(out)\n",
        "    #input()\n",
        "    '''if categories_path is not None:\n",
        "        category_fp = open(categories_path)\n",
        "        next(category_fp) # ignore header\n",
        "        revid2topic = {\n",
        "            l.strip().split(',')[0]: [float(x) for x in l.strip().split(',')[1:]]\n",
        "            for l in category_fp\n",
        "        }'''\n",
        "    for i, (line) in enumerate(tqdm(open(data_path))):\n",
        "        parts = line.strip().split('\\t')\n",
        "        #print(parts, len(parts))\n",
        "        #input(\"continue....\")\n",
        "\n",
        "        # if there pos/rel info\n",
        "        if len(parts) == 7:\n",
        "            [revid, pre, post, _, _, pos, rels] = parts\n",
        "        # no pos/rel info\n",
        "        elif len(parts) == 5:\n",
        "            [revid, pre, post, _, _] = parts\n",
        "            pos = ' '.join(['<UNK>'] * len(pre.strip().split()))\n",
        "            rels = ' '.join(['<UNK>'] * len(pre.strip().split()))\n",
        "        # broken line\n",
        "        else:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # break up tokens\n",
        "        tokens = pre.strip().split()\n",
        "        post_tokens = post.strip().split()\n",
        "        rels = rels.strip().split()\n",
        "        pos = pos.strip().split()\n",
        "\n",
        "        # get diff + binary diff masks\n",
        "        tok_diff = diff(tokens, post_tokens)\n",
        "        pre_tok_labels, post_tok_labels = get_tok_labels(tok_diff)\n",
        "                   \n",
        "        # make sure everything lines up    \n",
        "        if len(tokens) != len(pre_tok_labels) \\\n",
        "            or len(tokens) != len(rels) \\\n",
        "            or len(tokens) != len(pos) \\\n",
        "            or len(post_tokens) != len(post_tok_labels):\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # leave room in the post for start/stop and possible category/class token\n",
        "        if len(tokens) > max_seq_len - 1 or len(post_tokens) > max_seq_len - 1:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # category info if provided\n",
        "        # TODO -- if provided but not in diyi's data, we fill with random...is that ok?\n",
        "        '''if categories_path is not None and revid in revid2topic:\n",
        "            categories = revid2topic[revid]\n",
        "        else:'''\n",
        "        categories = np.random.uniform(size=43)   # 43 = number of categories\n",
        "        categories = categories / sum(categories) # normalize\n",
        "\n",
        "        '''if ARGS.category_input:\n",
        "            category_id = np.argmax(categories)\n",
        "            tokens = ['[unused%d]' % category_id] + tokens\n",
        "            pre_tok_labels = [EDIT_TYPE2ID['mask']] + pre_tok_labels\n",
        "            post_tok_labels = [EDIT_TYPE2ID['mask']] + post_tok_labels\n",
        "        '''\n",
        "\n",
        "        # add start + end symbols to post in/out\n",
        "        post_input_tokens = ['行'] + post_tokens\n",
        "        post_output_tokens = post_tokens + ['止'] \n",
        "\n",
        "        # shuffle + convert to ids + pad\n",
        "        try:\n",
        "            if noise:\n",
        "                pre_toks = noise_seq(\n",
        "                    tokens[:], \n",
        "                    drop_prob=ARGS.noise_prob, \n",
        "                    shuf_dist=ARGS.shuf_dist,\n",
        "                    drop_set=drop_set,\n",
        "                    keep_bigrams=ARGS.keep_bigrams)\n",
        "            else:\n",
        "                pre_toks = tokens\n",
        "\n",
        "            pre_ids = pad([tok2id[x] for x in pre_toks], 0)\n",
        "            post_in_ids = pad([tok2id[x] for x in post_input_tokens], 0)\n",
        "            post_out_ids = pad([tok2id[x] for x in post_output_tokens], 0)\n",
        "            pre_tok_label_ids = pad(pre_tok_labels, EDIT_TYPE2ID['mask'])\n",
        "            post_tok_label_ids = pad(post_tok_labels, EDIT_TYPE2ID['mask'])\n",
        "            rel_ids = pad([REL2ID.get(x, REL2ID['<UNK>']) for x in rels], 0)\n",
        "            pos_ids = pad([POS2ID.get(x, POS2ID['<UNK>']) for x in pos], 0)\n",
        "        except KeyError:\n",
        "            # TODO FUCK THIS ENCODING BUG!!!\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        input_mask = pad([0] * len(tokens), 1)\n",
        "        pre_len = len(tokens)\n",
        "\n",
        "        out['pre_ids'].append(pre_ids)\n",
        "        out['pre_masks'].append(input_mask)\n",
        "        out['pre_lens'].append(pre_len)\n",
        "        out['post_in_ids'].append(post_in_ids)\n",
        "        out['post_out_ids'].append(post_out_ids)\n",
        "        out['pre_tok_label_ids'].append(pre_tok_label_ids)\n",
        "        out['post_tok_label_ids'].append(post_tok_label_ids)\n",
        "        out['rel_ids'].append(rel_ids)\n",
        "        out['pos_ids'].append(pos_ids)\n",
        "        out['categories'].append(categories)\n",
        "\n",
        "    print('SKIPPED ', skipped)\n",
        "    return out\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivb9wEC7DwF6"
      },
      "source": [
        "# get data loader for data in data_path\n",
        "def get_dataloader(data_path, tok2id, batch_size, \n",
        "                   pickle_path=None, test=False, noise=False, add_del_tok=False, \n",
        "                   categories_path=None, sort_batch=True):\n",
        "    #global ARGS\n",
        "\n",
        "    def collate(data):\n",
        "        if sort_batch:\n",
        "            # sort by length for packing/padding\n",
        "            data.sort(key=lambda x: x[2], reverse=True)\n",
        "        # group by datatype\n",
        "        [\n",
        "            src_id, src_mask, src_len, \n",
        "            post_in_id, post_out_id, \n",
        "            pre_tok_label, post_tok_label,\n",
        "            rel_ids, pos_ids, categories\n",
        "        ] = [torch.stack(x) for x in zip(*data)]\n",
        "\n",
        "        # cut off at max len of this batch for unpacking/repadding\n",
        "        max_len = max(src_len)\n",
        "        data = [\n",
        "            src_id[:, :max_len], src_mask[:, :max_len], src_len, \n",
        "            post_in_id[:, :max_len+10], post_out_id[:, :max_len+10],    # +10 for wiggle room\n",
        "            pre_tok_label[:, :max_len], post_tok_label[:, :max_len+10], # +10 for post_toks_labels too (it's just gonna be matched up with post ids)\n",
        "            rel_ids[:, :max_len], pos_ids[:, :max_len], categories\n",
        "        ]\n",
        "\n",
        "        return data\n",
        "\n",
        "    if pickle_path is not None and os.path.exists(pickle_path):\n",
        "        print(\"pickle file exists!\")\n",
        "        examples = pickle.load(open(pickle_path, 'rb'))\n",
        "    else:\n",
        "        examples = get_examples(\n",
        "            data_path=data_path, \n",
        "            tok2id=tok2id,\n",
        "            max_seq_len=80, #ARGS.max_seq_len,\n",
        "            noise=False, #noise,\n",
        "            add_del_tok=False, #add_del_tok,\n",
        "            categories_path=None)#categories_path)\n",
        "\n",
        "        pickle.dump(examples, open(pickle_path, 'wb'))\n",
        "\n",
        "    data = TensorDataset(\n",
        "        torch.tensor(examples['pre_ids'], dtype=torch.long),\n",
        "        torch.tensor(examples['pre_masks'], dtype=torch.uint8), # byte for masked_fill()\n",
        "        torch.tensor(examples['pre_lens'], dtype=torch.long),\n",
        "        torch.tensor(examples['post_in_ids'], dtype=torch.long),\n",
        "        torch.tensor(examples['post_out_ids'], dtype=torch.long),\n",
        "        torch.tensor(examples['pre_tok_label_ids'], dtype=torch.float),  # for compartin to enrichment stuff\n",
        "        torch.tensor(examples['post_tok_label_ids'], dtype=torch.float),  # for loss multiplying\n",
        "        torch.tensor(examples['rel_ids'], dtype=torch.long),\n",
        "        torch.tensor(examples['pos_ids'], dtype=torch.long),\n",
        "        torch.tensor(examples['categories'], dtype=torch.float))\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        data,\n",
        "        sampler=(SequentialSampler(data) if test else RandomSampler(data)),\n",
        "        collate_fn=collate,\n",
        "        batch_size=batch_size)\n",
        "\n",
        "    return dataloader, len(examples['pre_ids'])"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoQGZRQAJ6Xj"
      },
      "source": [
        "#from shared.args import ARGS \n",
        "#from shared.constants import CUDA \n",
        "#import seq2seq.model as seq2seq_model\n",
        "CUDA = (torch.cuda.device_count() > 0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFt_ZGseLQ6A",
        "outputId": "27af68e0-6461-426d-ce02-d914c572dc5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO9jTo8oRRtJ"
      },
      "source": [
        "## Update the experiments directory\n",
        "DATA_DIRECTORY = '/content/drive/Shared drives/EC463 464 Senior Design Project/data/'\n",
        "LEXICON_DIRECTORY = DATA_DIRECTORY + 'lexicons/'\n",
        "PRYZANT_DATA = DATA_DIRECTORY + 'pryzant_data/WNC/'\n",
        "#IMPORTS = \n",
        "training_data = PRYZANT_DATA + 'biased.word.train'\n",
        "testing_data = PRYZANT_DATA + 'biased.word.test'\n",
        "categories_file = PRYZANT_DATA + 'revision_topics.csv'\n",
        "pickle_directory = '/content/drive/Shared drives/EC463 464 Senior Design Project/data/pickle_data/'\n",
        "cache_dir = DATA_DIRECTORY + 'cache/'\n",
        "model_save_dir = '/content/drive/Shared drives/EC463 464 Senior Design Project/models/'"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmyALBDnX7fl"
      },
      "source": [
        "# Load imports\n",
        "!cp '/content/drive/Shared drives/EC463 464 Senior Design Project/imports/data.py' .\n",
        "import data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjeba1jjaol3",
        "outputId": "24ff664a-4755-44ee-8aee-d9ca7ee59d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4upcahxZXBv",
        "outputId": "66a82d7b-4166-4c69-cb49-3d0b5e8989e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('LOADING DATA...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', os.getcwd() + '/cache')\n",
        "tok2id = tokenizer.vocab\n",
        "tok2id['<del>'] = len(tok2id)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/231508 [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LOADING DATA...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 231508/231508 [00:00<00:00, 4211677.71B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tbgzf4_bY7F",
        "outputId": "5d1905e4-c766-4432-ede5-a3dd8b1fc34b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataloader, num_train_examples = get_dataloader(\n",
        "    data_path=training_data,\n",
        "    tok2id=tok2id,\n",
        "    batch_size=32,\n",
        "    pickle_path=pickle_directory + 'train_data4.p',\n",
        "    categories_path=None #categories_file\n",
        "  )\n",
        "\n",
        "eval_dataloader, num_eval_examples = get_dataloader(\n",
        "    data_path=testing_data,\n",
        "    tok2id=tok2id,\n",
        "    batch_size=32,\n",
        "    pickle_path=pickle_directory + 'test_data4.p',\n",
        "    categories_path=None #categories_file\n",
        "  )\n",
        "\n",
        "print(num_train_examples, num_eval_examples)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "437it [00:00, 4362.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "980it [00:00, 4635.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1525it [00:00, 4852.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1978it [00:00, 4748.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2573it [00:00, 5053.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3188it [00:00, 5260.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3759it [00:00, 5386.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4269it [00:00, 5288.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4878it [00:00, 5505.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5468it [00:01, 5617.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6023it [00:01, 5352.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6556it [00:01, 5258.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7081it [00:01, 5049.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7587it [00:01, 5049.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "8203it [00:01, 5336.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "8815it [00:01, 5548.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "9376it [00:01, 5172.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "9918it [00:01, 5241.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "10449it [00:02, 3323.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "11043it [00:02, 3829.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "11519it [00:02, 4009.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "11988it [00:02, 4174.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12489it [00:02, 4393.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "12971it [00:02, 4513.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "13476it [00:02, 4660.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14068it [00:02, 4976.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "14591it [00:02, 5049.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15111it [00:03, 4907.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "15664it [00:03, 5076.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16271it [00:03, 5337.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16815it [00:03, 5125.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17337it [00:03, 5089.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "17938it [00:03, 5332.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "18481it [00:03, 5360.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "19060it [00:03, 5481.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "19613it [00:03, 5354.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "20153it [00:04, 5053.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "20665it [00:04, 4838.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "21156it [00:04, 2880.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "21702it [00:04, 3354.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "22263it [00:04, 3814.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "22736it [00:04, 4035.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "23285it [00:04, 4384.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "23880it [00:04, 4759.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "24405it [00:05, 4776.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "24953it [00:05, 4965.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "25476it [00:05, 4994.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "26041it [00:05, 5169.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "26573it [00:05, 4860.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "27073it [00:05, 4782.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "27664it [00:05, 5072.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "28183it [00:05, 5091.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "28745it [00:05, 5238.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "29276it [00:06, 4963.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "29781it [00:06, 4938.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "30346it [00:06, 5131.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "30865it [00:06, 5092.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "31427it [00:06, 5238.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32014it [00:06, 5412.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32624it [00:06, 5598.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "33189it [00:07, 2926.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "33750it [00:07, 3415.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "34352it [00:07, 3924.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "34887it [00:07, 4264.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "35406it [00:07, 4334.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "35941it [00:07, 4595.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "36537it [00:07, 4932.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "37073it [00:07, 4827.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "37622it [00:07, 5008.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "38175it [00:08, 5148.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "38707it [00:08, 4991.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "39219it [00:08, 4698.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "39788it [00:08, 4956.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "40305it [00:08, 5018.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "40918it [00:08, 5296.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "41493it [00:08, 5424.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "42044it [00:08, 5201.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "42572it [00:08, 5085.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "43087it [00:08, 4959.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "43688it [00:09, 5232.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "44233it [00:09, 5294.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "44780it [00:09, 5342.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "45337it [00:09, 5408.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "45881it [00:09, 5386.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "46440it [00:09, 5443.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "46986it [00:09, 5239.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "47513it [00:09, 5198.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "48035it [00:10, 2337.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "48515it [00:10, 2762.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "48979it [00:10, 3142.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "49576it [00:10, 3662.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "50059it [00:10, 3944.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "50629it [00:10, 4346.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "51140it [00:10, 4549.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "51660it [00:11, 4724.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "52274it [00:11, 5074.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "52817it [00:11, 4876.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "53803it [00:11, 4720.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SKIPPED  1503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1000it [00:00, 5690.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SKIPPED  32\n",
            "52300 968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnNtPufYk8QV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1hNyN2KlVWJ"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11a9sxzIlUvA"
      },
      "source": [
        "# BERT initialization params\n",
        "config = 'bert-base-uncased'\n",
        "cls_num_labels = 43\n",
        "tok_num_labels = 3\n",
        "tok2id = tok2id\n",
        "\n",
        "class BertForMultitask(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config, cls_num_labels=2, tok_num_labels=2, tok2id=None):\n",
        "        super(BertForMultitask, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        self.cls_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.cls_classifier = nn.Linear(config.hidden_size, cls_num_labels)\n",
        "        \n",
        "        self.tok_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.tok_classifier = nn.Linear(config.hidden_size, tok_num_labels)\n",
        "        \n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
        "        labels=None, rel_ids=None, pos_ids=None, categories=None, pre_len=None):\n",
        "        global ARGS\n",
        "        sequence_output, pooled_output = self.bert(\n",
        "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "\n",
        "        cls_logits = self.cls_classifier(pooled_output)\n",
        "        cls_logits = self.cls_dropout(cls_logits)\n",
        "\n",
        "        # NOTE -- dropout is after proj, which is non-standard\n",
        "        tok_logits = self.tok_classifier(sequence_output)\n",
        "        tok_logits = self.tok_dropout(tok_logits)\n",
        "\n",
        "        return cls_logits, tok_logits"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdN7lAcBoTl8"
      },
      "source": [
        "# define model!!\n",
        "model = BertForMultitask.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    cls_num_labels=cls_num_labels,\n",
        "    tok_num_labels=tok_num_labels, \n",
        "    cache_dir=cache_dir,\n",
        "    tok2id=tok2id)\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHD1X0rlbfuf"
      },
      "source": [
        "def build_optimizer(model, num_train_steps, learning_rate):\n",
        "    #global ARGS\n",
        "\n",
        "    '''if ARGS.tagger_from_debiaser:\n",
        "        parameters = list(model.cls_classifier.parameters()) + list(\n",
        "            model.tok_classifier.parameters())\n",
        "        parameters = list(filter(lambda p: p.requires_grad, parameters))\n",
        "        return optim.Adam(parameters, lr=ARGS.learning_rate)\n",
        "    else:'''\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    param_optimizer = list(filter(lambda name_param: name_param[1].requires_grad, param_optimizer))\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "    return BertAdam(optimizer_grouped_parameters,\n",
        "                          lr=learning_rate,\n",
        "                          warmup=0.1,\n",
        "                          t_total=num_train_steps)\n",
        "\n",
        "\n",
        "def build_loss_fn(debias_weight=None):\n",
        "    global ARGS\n",
        "    \n",
        "    if debias_weight is None:\n",
        "        debias_weight = 1 # default #ARGS.debias_weight\n",
        "    \n",
        "    weight_mask = torch.ones(3) #ARGS.num_tok_labels)\n",
        "    weight_mask[-1] = 0\n",
        "\n",
        "    if CUDA:\n",
        "        weight_mask = weight_mask.cuda()\n",
        "        criterion = CrossEntropyLoss(weight=weight_mask).cuda()\n",
        "        per_tok_criterion = CrossEntropyLoss(weight=weight_mask, reduction='none').cuda()\n",
        "    else:\n",
        "        criterion = CrossEntropyLoss(weight=weight_mask)\n",
        "        per_tok_criterion = CrossEntropyLoss(weight=weight_mask, reduction='none')\n",
        "\n",
        "\n",
        "    def cross_entropy_loss(logits, labels, apply_mask=None):\n",
        "        return criterion(\n",
        "            logits.contiguous().view(-1, 3), #ARGS.num_tok_labels), \n",
        "            labels.contiguous().view(-1).type('torch.cuda.LongTensor' if CUDA else 'torch.LongTensor'))\n",
        "\n",
        "    def weighted_cross_entropy_loss(logits, labels, apply_mask=None):\n",
        "        # weight mask = where to apply weight (post_tok_label_id from the batch)\n",
        "        weights = apply_mask.contiguous().view(-1)\n",
        "        weights = ((debias_weight - 1) * weights) + 1.0\n",
        "\n",
        "        per_tok_losses = per_tok_criterion(\n",
        "            logits.contiguous().view(-1, 3), # ARGS.num_tok_labels), \n",
        "            labels.contiguous().view(-1).type('torch.cuda.LongTensor' if CUDA else 'torch.LongTensor'))\n",
        "        per_tok_losses = per_tok_losses * weights\n",
        "\n",
        "        loss = torch.mean(per_tok_losses[torch.nonzero(per_tok_losses)].squeeze())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    if debias_weight == 1.0:\n",
        "        loss_fn = cross_entropy_loss\n",
        "    else:\n",
        "        loss_fn = weighted_cross_entropy_loss\n",
        "\n",
        "    return loss_fn"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qACcp-VcZlW"
      },
      "source": [
        "epochs = 4\n",
        "train_batch_size = 32 \n",
        "learning_rate = 3e-5\n",
        "optimizer = build_optimizer(\n",
        "    model, int((num_train_examples * epochs) / train_batch_size),\n",
        "    learning_rate)\n",
        "loss_fn = build_loss_fn()"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qm_wjW-ePZY"
      },
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter(model_save_dir)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKmDWrzGfzCv"
      },
      "source": [
        "def to_probs(logits, lens):\n",
        "    per_tok_probs = softmax(np.array(logits)[:, :, :2], axis=2)\n",
        "    pos_scores = per_tok_probs[:, :, -1]\n",
        "    \n",
        "    out = []\n",
        "    for score_seq, l in zip(pos_scores, lens):\n",
        "        out.append(score_seq[:l].tolist())\n",
        "    return out\n",
        "\n",
        "def run_inference(model, eval_dataloader, loss_fn, tokenizer):\n",
        "    #global ARGS\n",
        "\n",
        "    out = {\n",
        "        'input_toks': [],\n",
        "        'post_toks': [],\n",
        "\n",
        "        'tok_loss': [],\n",
        "        'tok_logits': [],\n",
        "        'tok_probs': [],\n",
        "        'tok_labels': [],\n",
        "\n",
        "        'labeling_hits': []\n",
        "    }\n",
        "\n",
        "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "        #if False and step > 2:\n",
        "        #    continue\n",
        "\n",
        "        if CUDA:\n",
        "            batch = tuple(x.cuda() for x in batch)\n",
        "\n",
        "        ( \n",
        "            pre_id, pre_mask, pre_len, \n",
        "            post_in_id, post_out_id, \n",
        "            tok_label_id, _,\n",
        "            rel_ids, pos_ids, categories\n",
        "        ) = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, tok_logits = model(pre_id, attention_mask=1.0-pre_mask,\n",
        "                rel_ids=rel_ids, pos_ids=pos_ids, categories=categories,\n",
        "                pre_len=pre_len)\n",
        "            tok_loss = loss_fn(tok_logits, tok_label_id, apply_mask=tok_label_id)\n",
        "        out['input_toks'] += [tokenizer.convert_ids_to_tokens(seq) for seq in pre_id.cpu().numpy()]\n",
        "        out['post_toks'] += [tokenizer.convert_ids_to_tokens(seq) for seq in post_in_id.cpu().numpy()]\n",
        "        out['tok_loss'].append(float(tok_loss.cpu().numpy()))\n",
        "        logits = tok_logits.detach().cpu().numpy()\n",
        "        labels = tok_label_id.cpu().numpy()\n",
        "        out['tok_logits'] += logits.tolist()\n",
        "        out['tok_labels'] += labels.tolist()\n",
        "        out['tok_probs'] += to_probs(logits, pre_len)\n",
        "        out['labeling_hits'] += tag_hits(logits, labels)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7simTvfvgGKD"
      },
      "source": [
        "def train_for_epoch(model, train_dataloader, loss_fn, optimizer):\n",
        "    global ARGS\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        #if ARGS.debug_skip and step > 2:\n",
        "        #    continue\n",
        "    \n",
        "        if CUDA:\n",
        "            batch = tuple(x.cuda() for x in batch)\n",
        "        ( \n",
        "            pre_id, pre_mask, pre_len, \n",
        "            post_in_id, post_out_id, \n",
        "            tok_label_id, _,\n",
        "            rel_ids, pos_ids, categories\n",
        "        ) = batch\n",
        "        _, tok_logits = model(pre_id, attention_mask=1.0-pre_mask,\n",
        "            rel_ids=rel_ids, pos_ids=pos_ids, categories=categories,\n",
        "            pre_len=pre_len)\n",
        "        loss = loss_fn(tok_logits, tok_label_id, apply_mask=tok_label_id)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    return losses\n",
        "\n",
        "def is_ranking_hit(probs, labels, top=1):\n",
        "    global ARGS\n",
        "    \n",
        "    # get rid of padding idx\n",
        "    [probs, labels] = list(zip(*[(p, l)  for p, l in zip(probs, labels) if l != 3 - 1 ]))\n",
        "    probs_indices = list(zip(np.array(probs)[:, 1], range(len(labels))))\n",
        "    [_, top_indices] = list(zip(*sorted(probs_indices, reverse=True)[:top]))\n",
        "    if sum([labels[i] for i in top_indices]) > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def tag_hits(logits, tok_labels, top=1):\n",
        "    #global ARGS\n",
        "    \n",
        "    probs = softmax(np.array(logits)[:, :, : 3 - 1], axis=2)\n",
        "\n",
        "    hits = [\n",
        "        is_ranking_hit(prob_dist, tok_label, top=top) \n",
        "        for prob_dist, tok_label in zip(probs, tok_labels)\n",
        "    ]\n",
        "    return hits"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1A4A0Pc13H",
        "outputId": "18c6e5e0-7e4d-4e82-b423-1be235505831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TRAIN MODEL!!\n",
        "# run_inference\n",
        "# train_for_epoch\n",
        "\n",
        "print('INITIAL EVAL...')\n",
        "model.eval()\n",
        "results = run_inference(model, eval_dataloader, loss_fn, tokenizer)\n",
        "writer.add_scalar('eval/tok_loss', np.mean(results['tok_loss']), 0)\n",
        "writer.add_scalar('eval/tok_acc', np.mean(results['labeling_hits']), 0)\n",
        "\n",
        "print('TRAINING...')\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    print('STARTING EPOCH ', epoch)\n",
        "    losses = train_for_epoch(model, train_dataloader, loss_fn, optimizer)\n",
        "    writer.add_scalar('train/loss', np.mean(losses), epoch + 1)\n",
        "\n",
        "        # eval\n",
        "    print('EVAL...')\n",
        "    model.eval()\n",
        "    results = run_inference(model, eval_dataloader, loss_fn, tokenizer)\n",
        "    writer.add_scalar('eval/tok_loss', np.mean(results['tok_loss']), epoch + 1)\n",
        "    writer.add_scalar('eval/tok_acc', np.mean(results['labeling_hits']), epoch + 1)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print('SAVING...')\n",
        "    torch.save(model.state_dict(), model_save_dir + 'model_%d.ckpt' % epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INITIAL EVAL...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 1/31 [00:07<03:43,  7.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 2/31 [00:14<03:33,  7.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 3/31 [00:22<03:30,  7.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 4/31 [00:30<03:25,  7.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 5/31 [00:37<03:18,  7.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 6/31 [00:44<03:02,  7.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 7/31 [00:51<02:55,  7.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 8/31 [00:59<02:48,  7.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 9/31 [01:05<02:35,  7.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 10/31 [01:13<02:31,  7.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 11/31 [01:19<02:21,  7.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▊      | 12/31 [01:27<02:15,  7.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 13/31 [01:35<02:13,  7.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 14/31 [01:42<02:04,  7.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 15/31 [01:49<01:56,  7.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 16/31 [01:55<01:44,  6.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▍    | 17/31 [02:03<01:38,  7.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 18/31 [02:08<01:23,  6.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████▏   | 19/31 [02:15<01:20,  6.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▍   | 20/31 [02:21<01:11,  6.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 21/31 [02:27<01:03,  6.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 22/31 [02:35<01:01,  6.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 74%|███████▍  | 23/31 [02:43<00:57,  7.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 77%|███████▋  | 24/31 [02:50<00:49,  7.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 81%|████████  | 25/31 [02:55<00:39,  6.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 84%|████████▍ | 26/31 [03:03<00:34,  6.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 87%|████████▋ | 27/31 [03:10<00:28,  7.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 28/31 [03:18<00:21,  7.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 94%|█████████▎| 29/31 [03:24<00:14,  7.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 97%|█████████▋| 30/31 [03:32<00:07,  7.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 31/31 [03:34<00:00,  6.90s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1635 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAINING...\n",
            "STARTING EPOCH  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 1/1635 [00:23<10:52:16, 23.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 2/1635 [00:46<10:37:03, 23.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 3/1635 [01:11<10:50:51, 23.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 4/1635 [01:35<10:51:21, 23.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 5/1635 [02:00<11:03:53, 24.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 6/1635 [02:19<10:18:28, 22.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 7/1635 [02:43<10:26:54, 23.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 8/1635 [03:10<10:56:28, 24.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 9/1635 [03:36<11:07:36, 24.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 10/1635 [03:56<10:36:41, 23.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 11/1635 [04:22<10:49:38, 24.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 12/1635 [04:44<10:34:59, 23.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 13/1635 [05:08<10:40:34, 23.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 14/1635 [05:32<10:41:18, 23.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 15/1635 [05:58<11:02:37, 24.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 16/1635 [06:23<11:03:44, 24.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 17/1635 [06:48<11:07:30, 24.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 18/1635 [07:12<11:00:39, 24.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 19/1635 [07:38<11:08:38, 24.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 20/1635 [08:02<11:06:56, 24.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 21/1635 [08:28<11:10:50, 24.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 22/1635 [08:51<10:58:05, 24.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 23/1635 [09:11<10:18:35, 23.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 24/1635 [09:32<10:08:52, 22.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 25/1635 [09:56<10:17:21, 23.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 26/1635 [10:22<10:35:28, 23.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 27/1635 [10:48<10:57:42, 24.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 28/1635 [11:08<10:22:04, 23.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 29/1635 [11:33<10:35:21, 23.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 30/1635 [12:00<10:57:45, 24.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 31/1635 [12:20<10:21:06, 23.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 32/1635 [12:45<10:34:15, 23.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 33/1635 [13:10<10:44:57, 24.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}